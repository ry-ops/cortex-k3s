#!/usr/bin/env bash
# scripts/lib/vulnerability-aggregator.sh
# Vulnerability Aggregator Library
# Collects security scan results from all scan workers and generates portfolio-wide summary
#
# Features:
#   - Aggregates vulnerabilities from all scan worker directories
#   - Deduplicates by CVE ID
#   - Counts by severity (critical, high, medium, low)
#   - Calculates portfolio risk score
#   - Identifies top vulnerable packages and CVEs
#
# Usage:
#   source "$CORTEX_HOME/scripts/lib/vulnerability-aggregator.sh"
#   aggregate_vulnerabilities
#   generate_summary
#   write_summary

set -euo pipefail

# ============================================================================
# Configuration
# ============================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CORTEX_HOME="${CORTEX_HOME:-$(cd "$SCRIPT_DIR/../.." && pwd)}"

# Directories and files
WORKERS_DIR="$CORTEX_HOME/agents/workers"
METRICS_DIR="$CORTEX_HOME/coordination/metrics"
SUMMARY_FILE="$METRICS_DIR/vulnerability-summary.json"
HISTORY_FILE="$METRICS_DIR/vulnerability-history.jsonl"

# Temporary storage for aggregation
AGGREGATION_TMP="/tmp/vuln-aggregation-$$"

# Severity weights for risk calculation
WEIGHT_CRITICAL=10
WEIGHT_HIGH=5
WEIGHT_MEDIUM=2
WEIGHT_LOW=1

# Ensure directories exist
mkdir -p "$METRICS_DIR"

# ============================================================================
# Logging
# ============================================================================

log_vuln() {
    local level="$1"
    shift
    echo "[$(date +%Y-%m-%dT%H:%M:%S%z)] [VULN-AGG] [$level] $*" >&2
}

# ============================================================================
# aggregate_vulnerabilities: Main aggregation function
# Scans worker directories and collects all vulnerability data
# Returns: JSON with aggregated vulnerabilities
# ============================================================================
aggregate_vulnerabilities() {
    log_vuln "INFO" "Starting vulnerability aggregation"

    # Create temp directory for aggregation
    mkdir -p "$AGGREGATION_TMP"

    # Initialize counters
    local total_critical=0
    local total_high=0
    local total_medium=0
    local total_low=0
    local repos_scanned=0
    local repos_clean=0
    local total_compliance=0
    local last_scan=""

    # Initialize arrays for CVEs and packages
    > "$AGGREGATION_TMP/cves.jsonl"
    > "$AGGREGATION_TMP/packages.jsonl"
    > "$AGGREGATION_TMP/repo_data.jsonl"

    # Find all security audit reports
    local scan_reports=$(find "$WORKERS_DIR" -path "*/worker-scan-*/security-audit-report.json" -type f 2>/dev/null || true)

    if [ -z "$scan_reports" ]; then
        log_vuln "WARN" "No security audit reports found"
        echo '{"total":0,"by_severity":{"critical":0,"high":0,"medium":0,"low":0}}'
        rm -rf "$AGGREGATION_TMP"
        return 0
    fi

    # Process each report
    while IFS= read -r report_file; do
        if [ ! -f "$report_file" ]; then
            continue
        fi

        log_vuln "INFO" "Processing: $report_file"

        # Extract worker ID from path
        local worker_id=$(echo "$report_file" | grep -o 'worker-scan-[0-9]*')

        # Get scan timestamp
        local scan_time=$(jq -r '.completed_at // empty' "$report_file" 2>/dev/null || echo "")
        if [ -n "$scan_time" ]; then
            if [ -z "$last_scan" ] || [[ "$scan_time" > "$last_scan" ]]; then
                last_scan="$scan_time"
            fi
        fi

        # Extract vulnerability counts from npm_audit
        local npm_critical=$(jq -r '.npm_audit.vulnerabilities.critical // 0' "$report_file" 2>/dev/null || echo "0")
        local npm_high=$(jq -r '.npm_audit.vulnerabilities.high // 0' "$report_file" 2>/dev/null || echo "0")
        local npm_moderate=$(jq -r '.npm_audit.vulnerabilities.moderate // 0' "$report_file" 2>/dev/null || echo "0")
        local npm_low=$(jq -r '.npm_audit.vulnerabilities.low // 0' "$report_file" 2>/dev/null || echo "0")

        # Also extract from summary if available
        local summary_critical=$(jq -r '.summary.critical_issues // 0' "$report_file" 2>/dev/null || echo "0")
        local summary_high=$(jq -r '.summary.high_issues // 0' "$report_file" 2>/dev/null || echo "0")
        local summary_medium=$(jq -r '.summary.medium_issues // 0' "$report_file" 2>/dev/null || echo "0")
        local summary_low=$(jq -r '.summary.low_issues // 0' "$report_file" 2>/dev/null || echo "0")

        # Use maximum of npm_audit and summary counts
        local repo_critical=$((npm_critical > summary_critical ? npm_critical : summary_critical))
        local repo_high=$((npm_high > summary_high ? npm_high : summary_high))
        local repo_medium=$((npm_moderate > summary_medium ? npm_moderate : summary_medium))
        local repo_low=$((npm_low > summary_low ? npm_low : summary_low))

        # Add to totals
        total_critical=$((total_critical + repo_critical))
        total_high=$((total_high + repo_high))
        total_medium=$((total_medium + repo_medium))
        total_low=$((total_low + repo_low))

        ((repos_scanned++))

        # Check if repo is clean
        local repo_total=$((repo_critical + repo_high + repo_medium + repo_low))
        if [ "$repo_total" -eq 0 ]; then
            ((repos_clean++))
        fi

        # Extract compliance score (calculate from status)
        local overall_status=$(jq -r '.summary.overall_status // "UNKNOWN"' "$report_file" 2>/dev/null || echo "UNKNOWN")
        local compliance_score=0
        case "$overall_status" in
            "PASS") compliance_score=100 ;;
            "WARN") compliance_score=75 ;;
            "REVIEW") compliance_score=50 ;;
            "FAIL") compliance_score=25 ;;
            *) compliance_score=50 ;;
        esac
        total_compliance=$((total_compliance + compliance_score))

        # Store repo data for later analysis
        jq -nc \
            --arg worker_id "$worker_id" \
            --argjson critical "$repo_critical" \
            --argjson high "$repo_high" \
            --argjson medium "$repo_medium" \
            --argjson low "$repo_low" \
            --argjson compliance "$compliance_score" \
            --arg scan_time "$scan_time" \
            '{
                worker_id: $worker_id,
                vulnerabilities: {
                    critical: $critical,
                    high: $high,
                    medium: $medium,
                    low: $low
                },
                compliance_score: $compliance,
                scan_time: $scan_time
            }' >> "$AGGREGATION_TMP/repo_data.jsonl"

        # Extract CVEs if available (from secrets_scan findings)
        jq -c '.secrets_scan.findings[]? // empty' "$report_file" 2>/dev/null | while read -r finding; do
            local severity=$(echo "$finding" | jq -r '.severity // "INFO"')
            local file=$(echo "$finding" | jq -r '.file // "unknown"')
            local issue=$(echo "$finding" | jq -r '.issue // "unknown"')

            jq -nc \
                --arg severity "$severity" \
                --arg file "$file" \
                --arg issue "$issue" \
                --arg worker_id "$worker_id" \
                '{
                    severity: $severity,
                    file: $file,
                    issue: $issue,
                    worker_id: $worker_id
                }' >> "$AGGREGATION_TMP/cves.jsonl"
        done

        # Extract vulnerable packages from recommendations
        jq -c '.recommendations[]? // empty' "$report_file" 2>/dev/null | while read -r rec; do
            local priority=$(echo "$rec" | jq -r '.priority // "UNKNOWN"')
            local category=$(echo "$rec" | jq -r '.category // "unknown"')
            local recommendation=$(echo "$rec" | jq -r '.recommendation // ""')

            jq -nc \
                --arg priority "$priority" \
                --arg category "$category" \
                --arg recommendation "$recommendation" \
                --arg worker_id "$worker_id" \
                '{
                    priority: $priority,
                    category: $category,
                    recommendation: $recommendation,
                    worker_id: $worker_id
                }' >> "$AGGREGATION_TMP/packages.jsonl"
        done

    done <<< "$scan_reports"

    # Calculate average compliance
    local avg_compliance=0
    if [ "$repos_scanned" -gt 0 ]; then
        avg_compliance=$((total_compliance / repos_scanned))
    fi

    # Set last scan to now if not found
    if [ -z "$last_scan" ]; then
        last_scan=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    fi

    # Store aggregation results
    jq -nc \
        --argjson critical "$total_critical" \
        --argjson high "$total_high" \
        --argjson medium "$total_medium" \
        --argjson low "$total_low" \
        --argjson repos_scanned "$repos_scanned" \
        --argjson repos_clean "$repos_clean" \
        --argjson avg_compliance "$avg_compliance" \
        --arg last_scan "$last_scan" \
        '{
            by_severity: {
                critical: $critical,
                high: $high,
                medium: $medium,
                low: $low
            },
            repos_scanned: $repos_scanned,
            repos_clean: $repos_clean,
            avg_compliance: $avg_compliance,
            last_scan: $last_scan
        }' > "$AGGREGATION_TMP/aggregated.json"

    log_vuln "INFO" "Aggregation complete: $repos_scanned repos, $((total_critical + total_high + total_medium + total_low)) total vulnerabilities"

    cat "$AGGREGATION_TMP/aggregated.json"
}

# ============================================================================
# get_top_cves: Find most common CVEs affecting multiple repos
# Returns: Array of top 10 CVEs with counts
# ============================================================================
get_top_cves() {
    local limit="${1:-10}"

    if [ ! -f "$AGGREGATION_TMP/cves.jsonl" ] || [ ! -s "$AGGREGATION_TMP/cves.jsonl" ]; then
        echo '[]'
        return 0
    fi

    # Group findings by issue and count occurrences
    jq -s '
        group_by(.issue) |
        map({
            issue: .[0].issue,
            severity: .[0].severity,
            count: length,
            affected_workers: [.[].worker_id] | unique
        }) |
        sort_by(-.count) |
        .[:'"$limit"']
    ' "$AGGREGATION_TMP/cves.jsonl" 2>/dev/null || echo '[]'
}

# ============================================================================
# get_top_packages: Find most common vulnerable packages
# Returns: Array of top packages with counts
# ============================================================================
get_top_packages() {
    local limit="${1:-10}"

    if [ ! -f "$AGGREGATION_TMP/packages.jsonl" ] || [ ! -s "$AGGREGATION_TMP/packages.jsonl" ]; then
        echo '[]'
        return 0
    fi

    # Group by category and count
    jq -s '
        group_by(.category) |
        map({
            category: .[0].category,
            priority: .[0].priority,
            count: length,
            affected_workers: [.[].worker_id] | unique
        }) |
        sort_by(-.count) |
        .[:'"$limit"']
    ' "$AGGREGATION_TMP/packages.jsonl" 2>/dev/null || echo '[]'
}

# ============================================================================
# calculate_risk_score: Calculate portfolio risk score
# Weight: critical=10, high=5, medium=2, low=1
# Normalize to 0-100 scale (100 = no issues)
# Returns: Integer 0-100
# ============================================================================
calculate_risk_score() {
    if [ ! -f "$AGGREGATION_TMP/aggregated.json" ]; then
        echo "100"
        return 0
    fi

    local critical=$(jq -r '.by_severity.critical // 0' "$AGGREGATION_TMP/aggregated.json")
    local high=$(jq -r '.by_severity.high // 0' "$AGGREGATION_TMP/aggregated.json")
    local medium=$(jq -r '.by_severity.medium // 0' "$AGGREGATION_TMP/aggregated.json")
    local low=$(jq -r '.by_severity.low // 0' "$AGGREGATION_TMP/aggregated.json")

    # Calculate weighted score
    local weighted_score=$((
        (critical * WEIGHT_CRITICAL) +
        (high * WEIGHT_HIGH) +
        (medium * WEIGHT_MEDIUM) +
        (low * WEIGHT_LOW)
    ))

    # Normalize to 0-100 (100 = no issues)
    # Use a logarithmic scale to handle large numbers gracefully
    # Max penalty is 100 points
    local max_penalty=100
    local penalty=0

    if [ "$weighted_score" -gt 0 ]; then
        # Each 10 weighted points reduces score by ~10%
        # Score = 100 * e^(-weighted_score/50)
        # Simplified: penalty = min(100, weighted_score * 2)
        penalty=$((weighted_score * 2))
        if [ "$penalty" -gt "$max_penalty" ]; then
            penalty=$max_penalty
        fi
    fi

    local risk_score=$((100 - penalty))
    if [ "$risk_score" -lt 0 ]; then
        risk_score=0
    fi

    echo "$risk_score"
}

# ============================================================================
# generate_summary: Create summary JSON with all aggregated data
# Returns: Complete summary JSON
# ============================================================================
generate_summary() {
    log_vuln "INFO" "Generating vulnerability summary"

    # Ensure aggregation has been run
    if [ ! -f "$AGGREGATION_TMP/aggregated.json" ]; then
        aggregate_vulnerabilities > /dev/null
    fi

    # Get aggregated data
    local aggregated=$(cat "$AGGREGATION_TMP/aggregated.json")

    # Calculate totals
    local total_vulns=$(jq -r '
        .by_severity.critical + .by_severity.high + .by_severity.medium + .by_severity.low
    ' <<< "$aggregated")

    # Get top CVEs and packages
    local top_cves=$(get_top_cves 10)
    local top_packages=$(get_top_packages 10)

    # Calculate risk score
    local risk_score=$(calculate_risk_score)

    # Get other values
    local repos_scanned=$(jq -r '.repos_scanned // 0' <<< "$aggregated")
    local repos_clean=$(jq -r '.repos_clean // 0' <<< "$aggregated")
    local avg_compliance=$(jq -r '.avg_compliance // 0' <<< "$aggregated")
    local last_scan=$(jq -r '.last_scan' <<< "$aggregated")
    local by_severity=$(jq -c '.by_severity' <<< "$aggregated")

    # Build complete summary
    local summary=$(jq -nc \
        --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
        --argjson total_vulnerabilities "$total_vulns" \
        --argjson by_severity "$by_severity" \
        --argjson repositories_scanned "$repos_scanned" \
        --argjson repositories_clean "$repos_clean" \
        --argjson avg_compliance_score "$avg_compliance" \
        --argjson portfolio_risk_score "$risk_score" \
        --argjson top_cves "$top_cves" \
        --argjson top_packages "$top_packages" \
        --arg last_scan "$last_scan" \
        '{
            timestamp: $timestamp,
            summary: {
                total_vulnerabilities: $total_vulnerabilities,
                by_severity: $by_severity,
                repositories_scanned: $repositories_scanned,
                repositories_clean: $repositories_clean,
                avg_compliance_score: $avg_compliance_score,
                portfolio_risk_score: $portfolio_risk_score
            },
            top_cves: $top_cves,
            top_packages: $top_packages,
            last_scan: $last_scan
        }')

    log_vuln "INFO" "Summary generated: risk_score=$risk_score, repos=$repos_scanned"

    echo "$summary"
}

# ============================================================================
# write_summary: Write summary to coordination files
# Output: coordination/metrics/vulnerability-summary.json
# Append: coordination/metrics/vulnerability-history.jsonl
# ============================================================================
write_summary() {
    log_vuln "INFO" "Writing vulnerability summary to files"

    # Generate the summary
    local summary=$(generate_summary)

    # Write main summary file
    echo "$summary" | jq '.' > "$SUMMARY_FILE"
    log_vuln "INFO" "Written: $SUMMARY_FILE"

    # Append to history
    echo "$summary" >> "$HISTORY_FILE"
    log_vuln "INFO" "Appended to: $HISTORY_FILE"

    # Cleanup temp files
    rm -rf "$AGGREGATION_TMP"

    echo "$summary"
}

# ============================================================================
# get_vulnerability_trend: Get vulnerability trend over time
# Args: hours (default 24)
# Returns: JSON with trend data
# ============================================================================
get_vulnerability_trend() {
    local hours="${1:-24}"

    if [ ! -f "$HISTORY_FILE" ]; then
        echo '{"trend":"stable","data_points":0}'
        return 0
    fi

    local cutoff=$(date -v-${hours}H +%Y-%m-%dT%H:%M:%SZ 2>/dev/null || date -d "$hours hours ago" +%Y-%m-%dT%H:%M:%SZ)

    # Get recent entries and analyze trend
    local recent_entries=$(grep -c . "$HISTORY_FILE" || echo "0")

    if [ "$recent_entries" -lt 2 ]; then
        echo '{"trend":"stable","data_points":'"$recent_entries"'}'
        return 0
    fi

    # Compare first and last risk scores
    local first_score=$(head -1 "$HISTORY_FILE" | jq -r '.summary.portfolio_risk_score // 100')
    local last_score=$(tail -1 "$HISTORY_FILE" | jq -r '.summary.portfolio_risk_score // 100')

    local trend="stable"
    local diff=$((last_score - first_score))

    if [ "$diff" -gt 5 ]; then
        trend="improving"
    elif [ "$diff" -lt -5 ]; then
        trend="degrading"
    fi

    jq -nc \
        --arg trend "$trend" \
        --argjson first_score "$first_score" \
        --argjson last_score "$last_score" \
        --argjson data_points "$recent_entries" \
        '{
            trend: $trend,
            first_score: $first_score,
            last_score: $last_score,
            data_points: $data_points
        }'
}

# ============================================================================
# Cleanup function
# ============================================================================
cleanup_aggregation() {
    rm -rf "$AGGREGATION_TMP"
}

# Set trap for cleanup
trap cleanup_aggregation EXIT

# ============================================================================
# Export functions
# ============================================================================
export -f aggregate_vulnerabilities 2>/dev/null || true
export -f generate_summary 2>/dev/null || true
export -f write_summary 2>/dev/null || true
export -f get_top_cves 2>/dev/null || true
export -f calculate_risk_score 2>/dev/null || true
export -f get_vulnerability_trend 2>/dev/null || true

log_vuln "INFO" "Vulnerability aggregator library loaded"
